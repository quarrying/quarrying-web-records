#! https://www.zhihu.com/question/400739097/answer/1283992956

[comment]: <> (Answer URL: https://www.zhihu.com/question/400739097/answer/1283992956)
[comment]: <> (Question Title: 1*1的卷积核在实际计算时能否优化，达到更高的计算效率？)
[comment]: <> (Author Name: 采石工)
[comment]: <> (Create Time: 2020-06-15 12:22:14)

没有做过推理引擎的开发, 抛砖引玉, 强答一下 (针对核尺寸为 `1*1`, stride 为1, 没有 bias 项的一般卷积):

`1)` 如果输入 feature map 的尺寸为 [batch_size, rows, cols, in_channels], 卷积核尺寸为 [1, 1, in_channels, out_channels] ( TensorFlow 中即采用这种维度顺序), 则可以先将输入 feature map reshape 成 [batch_size*rows*cols, in_channels] (可能需要假设前面三维的内存是连续的), 卷积核 squeeze 成 [in_channels, out_channels], 然后就可以使用矩阵乘法计算得到尺寸为 [batch_size*rows*cols, out_channels] 的结果, 再将其 reshape 为 [batch_size, rows, cols, out_channels], 这个结果和直接卷积的结果应该是一样的.

注意: 一些包还支持张量乘 (如 NumPy 中的 `np.dot`  / `np.tensordot`, TensorFlow 中的 `tf.tensordot`, PyTorch 中的 `torch.matmul`). 这时可直接计算尺寸为 [batch_size, rows, cols, in_channels] 的输入 feature map 和尺寸为 [in_channels, out_channels] 的 squeeze 后的卷积核之间的张量乘/矩阵乘, 输出结果的尺寸为 [batch_size, rows, cols, out_channels]. 这样避免了reshape且无需假设前三维内存连续, 效率会更高.

`2)` 如果输入 feature map 的尺寸是 [batch_size, in_channels, rows, cols], 卷积核尺寸为 [out_channels, in_channels, 1, 1] (PyTorch 中即采用这种维度顺序), 则需先将 feature map 的维度顺序调整为 [batch_size, rows, cols, in_channels], 后面的做法就和 1) 一样了. 参考代码如下, **注意没有严格地进行耗时分析, 不能保证转换后的效率较高** .

```python
import time
import torch
import numpy as np

x_np = np.random.randn(64, 3, 224, 224)
x_pt = torch.from_numpy(x_np.astype(np.float32))

conv = torch.nn.Conv2d(3, 32, 1, bias=False)
# torch.nn.Conv2d
start_time = time.time()
conv_res = conv(x_pt)
print(time.time() - start_time)

# torch.matmul
start_time = time.time()
x_mm = x_pt.permute(0,2,3,1)
conv_weight = torch.squeeze(conv.weight)
mm_res = torch.matmul(x_mm, conv_weight.t())
mm_res = mm_res.permute(0,3,1,2)
print(time.time() - start_time)

print(torch.allclose(conv_res, mm_res))
```

