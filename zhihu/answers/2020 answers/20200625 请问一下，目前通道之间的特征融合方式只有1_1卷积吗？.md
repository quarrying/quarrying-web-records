#! https://www.zhihu.com/question/403348808/answer/1301852600

[comment]: <> (Answer URL: https://www.zhihu.com/question/403348808/answer/1301852600)
[comment]: <> (Question Title: 请问一下，目前通道之间的特征融合方式只有1*1卷积吗？)
[comment]: <> (Author Name: 采石工)
[comment]: <> (Create Time: 2020-06-25 15:12:42)
[comment]: <> "Channel Attention 为了计算各通道的权重, 会采用某种方式实现各通道间的信息传递 (如 SE Block); Layer Normalization 要计算空间和通道维度上的统计量 (均值和方差) 用来归一化, 有通道间的信息传递; cross channel 的 Local Response Normalization 中利用同一位置相邻通道的特征进行归一化, 也有通道间的信息传递."

默认题主问的是 "同一 feature map 中各通道特征之间的信息融合", 除了 1*1 卷积能想到的有:

`1)` Channel Pooling. [2018 ECCV] CBAM_ Convolutional Block Attention Module 论文中看到的一个概念, 即在通道维度进行池化操作 (一般的池化是在空间维度上做的). 

在 [2016] Learning a Discriminative Filter Bank within a CNN for Fine-grained Recognition 中也看到了类似的概念, 在文中称为: Cross-Channel Pooling, 用来合并同类不同 channel 的 feature map. 具体地, 在文中 Cross-Channel Pooling 输入尺寸为 $kM \times 1 \times 1$ 的 feature map (其中 $M$ 是类别数, $k$ 是每一类的 discriminative patch 数), 每 $k$ 个一组求平均值或最大值, 最后输出尺寸为 $M \times 1 \times 1$ 的 feature map. 感觉可以理解为: stride 等于 kernel_size 的 channel pooling (分组的 channel pooling). 而 [2018 ECCV] CBAM_ Convolutional Block Attention Module 中的 channel pooling 可以理解为 global channel pooling.

`2)` Pointwise group convolution + channel shuffle. [2017] ShuffleNet_ An Extremely Efficient Convolutional Neural Network for Mobile Devices 中提出的一个模块, Pointwise group convolution 即分组的 1x1 卷积. 这个模块相对于一般的 1x1 卷积, 具有更低的参数量和计算量, 同时利用后续的 channel shuffle 操作来促进不同组的通道间的信息流动, 不会带来太多精度损失.

`3)` Bilinear Pooling. 可参考 [2015] Bilinear CNNs for Fine-grained Visual Recognition. Bilinear Pooling 对通道数相同的两个 feature map (两个 feature map 可以有不同的来源, 也可以是完全一样的) 的每个位置的特征计算向量外积并 flatten. 如果两个 feature map 一样, 则这个步骤即相当于对这个 feature map 做了通道间的信息融合.

