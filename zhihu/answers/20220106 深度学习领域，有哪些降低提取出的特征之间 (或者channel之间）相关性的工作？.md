#! https://www.zhihu.com/answer/2300522635


[//]: # (Answer URL: https://www.zhihu.com/question/494598512)
[//]: # (深度学习领域，有哪些降低提取出的特征之间 (或者channel之间）相关性的工作？)
[//]: # (Author Name: https://www.zhihu.com/people/quarrying)

想到两种 "降低卷积层各滤波器之间冗余或相关性" 或 "降低 feature map 各通道之间冗余或相关性" 的方法:

### 1) CReLU

论文作者通过实验发现: CNN 中底层的卷积层的一些滤波器之间存在着负相关, 也就是说滤波器存在冗余. 为了减少这种冗余, 论文作者提出了 Concatenated Rectified Linear Units (CReLU), 其定义如下: 
$\operatorname{CReLU} \left( x \right) = \left( {{{\left[ x \right]}_ + },{{\left[ { - x} \right]}_ + }} \right)$, 其中 ${\left[  \cdot  \right]_ + } = \max \left( { \cdot ,0} \right)$.

### 2) Orthogonality Regularization

该正则化显式要求卷积层的各滤波器之间的尽量独立. 设某卷积层的权重为 $K \in \mathbb{R}^{N \times C \times H \times W}$, $N, C, H, W$ 分别表示滤波器的个数, 输入通道数, 高和宽, 首先将其 reshape 为: $K' \in \mathbb{R}^{N \times M}$, 其中 $M = C \times H \times W$, Orthogonality Regularization 的一种可能形式为: $\lambda \| K'^TK' - I \|_F^2$


参考:
- [2016 ICML] Understanding and Improving Convolutional Neural Networks via Concatenated Rectified Linear Units
- [2018] Can We Gain More from Orthogonality Regularizations in Training Deep CNNs

