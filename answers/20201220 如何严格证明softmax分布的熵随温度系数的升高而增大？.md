#! https://www.zhihu.com/question/424674028/answer/1636391262

[comment]: <> (Answer URL: https://www.zhihu.com/question/424674028/answer/1636391262)
[comment]: <> (Question Title: 如何严格证明softmax分布的熵随温度系数的升高而增大？)
[comment]: <> (Author Name: 采石工)
[comment]: <> (Create Time: 2020-12-20 15:31:12)

先列出softmax函数的定义:

$$\operatorname{softmax}(\vec x) = \frac{\left[\exp(x_1); \exp(x_2); \cdots;
\exp(x_n) \right]}{\sum_{i=1}^n \exp(x_i)} = \frac{{{{\exp }^ \circ }\left(
{\vec x} \right)}}{{{{\vec 1}^T}{{\exp }^ \circ }\left( {\vec x} \right)}}$$

以及softmax函数的熵的公式:

$\begin{aligned} \operatorname{entropy} (\vec x, \alpha) &=-
\operatorname{softmax}^T(\alpha\vec x)\log^\circ
\operatorname{softmax}(\alpha\vec x) \end{aligned}$  ,其中  $\alpha > 0$  ,
其是温度系数的倒数. 欲证明softmax函数的熵随温度系数的升高而增大, 即证明softmax函数的熵随着  $\alpha$  的增大而减少, 即证明
$\operatorname{entropy} (\vec x, \alpha)$  是关于  $\alpha$  的减函数. 于是下面计算
$\operatorname{entropy} (\vec x, \alpha)$  关于  $\alpha$  的导数.

在此之前先引入一些结论:

$$\begin{aligned} \mathrm{d}\operatorname{softmax} \left( {\vec x} \right) &=
\left(\operatorname{diag} \left( {\operatorname{softmax} \left( {\vec x}
\right)} \right) - \operatorname{softmax} \left( {\vec x}
\right){\operatorname{softmax} ^T}\left( {\vec x} \right)\right) \mathrm{d}
\vec x\\\ \end{aligned}$$

$\begin{aligned} \mathrm{d} \log^\circ \operatorname{softmax} \left( {\vec x}
\right) &= \left( {I - \vec 1{{\operatorname{softmax} }^T}\left( {\vec x}
\right)} \right)\mathrm{d}\vec x \end{aligned}$  , 其中  $I$  为全1矩阵.

它们的推导详见: https://zhuanlan.zhihu.com/p/147901986


所以

$$\begin{aligned} \mathrm{d} \operatorname{entropy} (\vec x, \alpha) &=
\mathrm{d} (-\operatorname{softmax}^T(\alpha\vec x )\log^\circ
\operatorname{softmax}(\alpha\vec x) ) \\\ &=- \log^{\circ T}
\operatorname{softmax}(\alpha\vec x) )\left(\operatorname{diag} \left(
{\operatorname{softmax} \left( {\alpha \vec x} \right)} \right) -
\operatorname{softmax} \left( {\alpha\vec x} \right){\operatorname{softmax}
^T}\left( {\alpha\vec x} \right)\right) \mathrm{d} (\alpha\vec x) \\\ &\-
\operatorname{softmax}^T(\alpha\vec x)\left( {I - \vec
1{{\operatorname{softmax} }^T}\left( {\alpha\vec x} \right)} \right)\mathrm{d}
(\alpha\vec x) \\\ &=- \log^{\circ T} \operatorname{softmax}(\alpha\vec x)
)\left(\operatorname{diag} \left( {\operatorname{softmax} \left( {\alpha \vec
x} \right)} \right) - \operatorname{softmax} \left( {\alpha\vec x}
\right){\operatorname{softmax} ^T}\left( {\alpha\vec x} \right)\right) \vec x
\mathrm{d} \alpha \\\ &\- \operatorname{softmax}^T(\alpha\vec x)\left( {I -
\vec 1{{\operatorname{softmax} }^T}\left( {\alpha\vec x} \right)} \right) \vec
x \mathrm{d}\alpha \end{aligned}$$

于是  $\begin{aligned} \frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha} &=- \log^{\circ T}
\operatorname{softmax}(\alpha\vec x) )\left(\operatorname{diag} \left(
{\operatorname{softmax} \left( {\alpha \vec x} \right)} \right) -
\operatorname{softmax} \left( {\alpha\vec x} \right){\operatorname{softmax}
^T}\left( {\alpha\vec x} \right)\right) \vec x \\\\-&
\operatorname{softmax}^T(\alpha\vec x)\left( {I - \vec
1{{\operatorname{softmax} }^T}\left( {\alpha\vec x} \right)} \right) \vec x
\end{aligned}$

因为  $\log^{\circ} \operatorname{softmax}(\alpha\vec x) =\alpha\vec x
+\operatorname{LSE}(\alpha\vec x)\vec{1}$  ,

$$\operatorname{softmax}^T(\alpha\vec x)\left( {I - \vec
1{{\operatorname{softmax} }^T}\left( {\alpha\vec x} \right)} \right) \vec x
=\left(\vec{1} -\operatorname{softmax}(\alpha\vec{x}) \right)^T\vec x$$

另外为了简化推导, 不妨记  $M=\operatorname{diag} \left( {\operatorname{softmax} \left(
{\alpha \vec x} \right)} \right) - \operatorname{softmax} \left( {\alpha\vec
x} \right){\operatorname{softmax} ^T}\left( {\alpha\vec x} \right)$

所以  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha} =- \left( \alpha\vec x
+\operatorname{LSE}(\alpha\vec x)\vec{1} \right)^T M\vec x - \left(\vec{1}
-\operatorname{softmax}(\alpha\vec{x}) \right)^T\vec x$  .

可以证明:  $\vec{1} ^T M\vec x = 0$  , 所以

$\frac{\mathrm{d}\operatorname{entropy} (\vec x, \alpha)}{\mathrm{d}\alpha} =
- \alpha\vec x ^T M\vec x - \left(\vec{1}
-\operatorname{softmax}(\alpha\vec{x}) \right)^T\vec x$  .

下面的思路: 只要能证明  $M$  是正定矩阵 (这时  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha}$  是关于  $\vec x$  的凹函数), 且
$\frac{\mathrm{d}\operatorname{entropy} (\vec x, \alpha)}{\mathrm{d}\alpha}$
的最大值小于0, 就能证明  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha} < 0$  ( 即  $\operatorname{entropy} (\vec x,
\alpha)$  是关于  $\alpha$  的减函数).

1) 欲证  $M$  是正定矩阵, 即证  $\operatorname{diag} \left( {\operatorname{softmax}
\left( { \vec x} \right)} \right) - \operatorname{softmax} \left( { \vec x}
\right){\operatorname{softmax} ^T}\left( { \vec x} \right)$  是正定矩阵.

$${{\vec v}^T}\left[ {{\mathop{\rm diag}\nolimits} \left( {{\mathop{\rm
softmax}\nolimits} \left( {\vec x} \right)} \right) - {\mathop{\rm
softmax}\nolimits} \left( {\vec x} \right){{{\mathop{\rm softmax}\nolimits}
}^T}\left( {\vec x} \right)} \right]\vec v\\\ = {{\vec v}^T}\frac{{{{\vec
1}^T}{{\exp }^ \circ }\left( {\vec x} \right){\mathop{\rm diag}\nolimits}
\left( {{{\exp }^ \circ }\left( {\vec x} \right)} \right) - {{\exp }^ \circ
}\left( {\vec x} \right){{\exp }^ \circ }\left( {{{\vec x}^T}}
\right)}}{{{{\left( {{{\vec 1}^T}{{\exp }^ \circ }\left( {\vec x} \right)}
\right)}^2}}}\vec v\\\ = \frac{{{{\vec 1}^T}{{\exp }^ \circ }\left( {\vec x}
\right){{\vec v}^T}{\mathop{\rm diag}\nolimits} \left( {{{\exp }^ \circ
}\left( {\vec x} \right)} \right)\vec v - {{\vec v}^T}{{\exp }^ \circ }\left(
{\vec x} \right){{\exp }^ \circ }\left( {{{\vec x}^T}} \right)\vec
v}}{{{{\left( {{{\vec 1}^T}{{\exp }^ \circ }\left( {\vec x} \right)}
\right)}^2}}}$$

上式中的  ${\vec v^T}{\exp ^ \circ }\left( {\vec x} \right){\exp ^ \circ }\left(
{{{\vec x}^T}} \right)\vec v = {\left( {\sum\nolimits_{i = 1}^n {{v_i}\exp
\left( {{x_i}} \right)} } \right)^2}$

$${\vec 1^T}{\exp ^ \circ }\left( {\vec x} \right){\vec v^T}{\mathop{\rm
diag}\nolimits} \left( {{{\exp }^ \circ }\left( {\vec x} \right)} \right)\vec
v = \left( {\sum\nolimits_{i = 1}^n {\exp \left( {{x_i}} \right)} }
\right)\left( {\sum\nolimits_{i = 1}^n {v_i^2\exp \left( {{x_i}} \right)} }
\right)$$

令  ${a_i} = {v_i}\sqrt {\exp \left( {{x_i}} \right)}$  和  ${b_i} = \sqrt {\exp
\left( {{x_i}} \right)}$  ，所以

$${{\vec v}^T}\left[ {{\mathop{\rm diag}\nolimits} \left( {{\mathop{\rm
softmax}\nolimits} \left( {\vec x} \right)} \right) - {\mathop{\rm
softmax}\nolimits} \left( {\vec x} \right){{{\mathop{\rm softmax}\nolimits}
}^T}\left( {\vec x} \right)} \right]\vec v = \frac{{\left( {\sum\nolimits_{i =
1}^n {b_i^2} } \right)\left( {\sum\nolimits_{i = 1}^n {a_i^2} } \right) -
{{\left( {\sum\nolimits_{i = 1}^n {{a_i}{b_i}} } \right)}^2}}}{{{{\left(
{{{\vec 1}^T}{{\exp }^ \circ }\left( {\vec x} \right)} \right)}^2}}}$$

根据 Cauchy-Schwarz不等式可得  ${{\vec v}^T}\left[ {{\mathop{\rm diag}\nolimits}
\left( {{\mathop{\rm softmax}\nolimits} \left( {\vec x} \right)} \right) -
{\mathop{\rm softmax}\nolimits} \left( {\vec x} \right){{{\mathop{\rm
softmax}\nolimits} }^T}\left( {\vec x} \right)} \right]\vec v \geq 0$  , 所以

$\operatorname{diag} \left( {\operatorname{softmax} \left( { \vec x} \right)}
\right) - \operatorname{softmax} \left( { \vec x}
\right){\operatorname{softmax} ^T}\left( { \vec x} \right)$  是半正定矩阵, 于是  $M$
是半正定矩阵 (与预期的略有不同, 期望证明  $M$  是正定矩阵).

2) 因为  $M$  是半正定矩阵, 所以  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha}$  有最大值, 下面求这个最大值.
$\frac{\mathrm{d^2}\operatorname{entropy} (\vec x, \alpha)}{\mathrm{d}\vec
x\mathrm{d}\alpha}=-2\alpha M \vec{x} +\alpha M\vec x -(1 -
\operatorname{softmax}(\alpha \vec x))$  , 令该式等于0可得  $-\alpha M\vec x = 1 -
\operatorname{softmax}(\alpha \vec x)$  , 将其代入
$\frac{\mathrm{d}\operatorname{entropy} (\vec x, \alpha)}{\mathrm{d}\alpha}$
, 可得  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha} = 0$  , 即  $\frac{\mathrm{d}\operatorname{entropy}
(\vec x, \alpha)}{\mathrm{d}\alpha}$  的最大值等于0 (与预期的也略有不同, 期望证明
$\frac{\mathrm{d}\operatorname{entropy} (\vec x, \alpha)}{\mathrm{d}\alpha}$
的最大值小于0).

综上:  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha} \leq 0$  . 即一般情况下, softmax函数的熵随着  $\alpha$  的增大而减少
(或者说随着温度系数的增大而增大), 但某些情况下 (即  $\frac{\mathrm{d}\operatorname{entropy} (\vec x,
\alpha)}{\mathrm{d}\alpha} = 0$  )时softmax函数的熵随着  $\alpha$  的增大而保持不变
(或者说随着温度系数的增大而保持不变).

* * *

  * 20201220 创建回答 

PS: 这个结论从10月份开始证明, 断断续续到今天才告一段落, 请大家指正!

PPS: 由上面的推导可见, 从理论上存在softmax函数的熵随着温度系数的增大而保持不变的可能性, 不知道在现实中是否有这种可能性.

