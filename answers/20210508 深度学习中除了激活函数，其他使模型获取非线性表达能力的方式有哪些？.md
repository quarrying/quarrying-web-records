#! https://www.zhihu.com/question/457890703/answer/1875565783

[comment]: <> (Answer URL: https://www.zhihu.com/question/457890703/answer/1875565783)
[comment]: <> (深度学习中除了激活函数，其他使模型获取非线性表达能力的方式有哪些？)
[comment]: <> (Author Name: https://www.zhihu.com/people/quarrying)

除了激活函数层, 最大值池化层也是非线性操作. 笔者用 PyTorch 写了一段小例程, 可以用来检验 `torch.nn.Module` 执行的是否为线性操作, 感兴趣的朋友不妨一试.

```python
"""
@author: quarryman
@date: 20210508
"""
import torch


def is_linear_op(func, featmap_size):
    x = torch.randn(featmap_size)
    y = torch.randn(featmap_size)
    kx = torch.rand(1)
    ky = torch.rand(1)
    # 利用线性变换的定义
    z1 = func(kx * x + ky * y)
    z2 = kx * func(x) + ky * func(y)
    return torch.allclose(z1, z2, atol=1e-06)


if __name__ == '__main__':
    in_channels = 128
    conv2d = torch.nn.Conv2d(in_channels, 2, 3, 3, bias=False)
    print(is_linear_op(conv2d, featmap_size=(3, in_channels, 7, 7)))
    conv2d = torch.nn.Conv2d(in_channels, 2, 3, 3, bias=True)
    print(is_linear_op(conv2d, featmap_size=(3, in_channels, 7, 7)))
    print('===========================')

    in_features = 128
    linear = torch.nn.Linear(in_features, 10, bias=False)
    print(is_linear_op(linear, featmap_size=(3, in_channels)))
    linear = torch.nn.Linear(in_features, 3, bias=True)
    print(is_linear_op(linear, featmap_size=(3, in_channels)))
    print('===========================')


    relu = torch.nn.ReLU()
    print(is_linear_op(relu, featmap_size=(4, 8, 7, 7)))
    sigmoid = torch.nn.Sigmoid()
    print(is_linear_op(sigmoid, featmap_size=(4, 8, 7, 7)))
    print('===========================')

    maxpool2d = torch.nn.MaxPool2d(2)
    print(is_linear_op(maxpool2d, featmap_size=(4, 8, 7, 7)))
    avgpool2d = torch.nn.AvgPool2d(2)
    print(is_linear_op(avgpool2d, featmap_size=(4, 8, 7, 7)))
    print('===========================')
```
输出结果为:
```python
True
False
===========================
True
False
===========================
False
False
===========================
False
True
===========================
```

