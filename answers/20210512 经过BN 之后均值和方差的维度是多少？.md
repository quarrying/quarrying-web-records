#! https://www.zhihu.com/question/458873425/answer/1882928002

[//]: # (Answer URL: https://www.zhihu.com/question/458873425/answer/1882928002)
[//]: # (经过BN 之后均值和方差的维度是多少？)
[//]: # (Author Name: https://www.zhihu.com/people/quarrying)


想要知道结论, 最有效的方法是自己写代码去验证.

```python
import torch


def print_named_parameters(model):
    for k, (name, param) in enumerate(model.named_parameters()):
        print('[{}] {:<25}: {}'.format(k+1, name, param.shape))
     

def print_named_buffers(model):
    for k, (name, module) in enumerate(model.named_buffers()):
        print('[{}] {:<25}: {}'.format(k+1, name, module.shape))
        

if __name__ == '__main__':
    num_batches, num_channels, height, width = 32, 16, 7, 7
    x = torch.randn(num_batches, num_channels, height, width)
    batchnorm2d = torch.nn.BatchNorm2d(num_channels)
    y = batchnorm2d(x)
    print(y.shape)
    print_named_parameters(batchnorm2d)
    print_named_buffers(batchnorm2d)

```
输出结果为:
```
torch.Size([32, 16, 7, 7])
[1] weight                   : torch.Size([16])
[2] bias                     : torch.Size([16])
[1] running_mean             : torch.Size([16])
[2] running_var              : torch.Size([16])
[3] num_batches_tracked      : torch.Size([])
```
**可见**: 尺寸为 `(N, C, H, W)` 的数据经过 BN 之后, 输出数据的维度仍然为 `(N, C, H, W)`; 均值和方差的维度为 `(C,)` (不同深度学习框架, 不同实现可能不一样, 比如还可能为 `(1, C, 1, 1)`); BN 是在 `N, H, W` 维度上计算均值和方差的, 所以是在 `N, H, W` 维度上做归一化.
