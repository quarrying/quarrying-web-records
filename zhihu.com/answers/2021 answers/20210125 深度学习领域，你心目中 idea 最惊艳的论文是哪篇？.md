#! https://www.zhihu.com/question/440729199/answer/1697379849

[comment]: <> (Answer URL: https://www.zhihu.com/question/440729199/answer/1697379849)
[comment]: <> (Question Title: 深度学习领域，你心目中 idea 最惊艳的论文是哪篇？)
[comment]: <> (Author Name: 采石工)
[comment]: <> (Create Time: 2021-01-25 20:07:06)

发现没人提 Batch Normalization  [1]  .

Batch Normalization 在 2015 年横空出世, 现在基本上已经是某些任务 (如分类, 分割和检测) 网络结构的标配. BN 可以起到加速收敛, 稳定训练和涨点的作用. 关于 BN 还有一些事实: BN 可以减少 Internal Covariate Shift; BN 是一种正则化方式, 可以代替 dropout; BN 的放置位置有一定的讲究  [2]  [3]  ; BN 对模型迁移有负面作用  [4]  等等. 但同时对 Batch Normalization 有效性的解释目前还在探索中  [5]  .

##  参考

  1. ^  [2015] Batch Normalization_ Accelerating Deep Network Training b y Reducing Internal Covariate Shift 
  2. ^  [2016 ECCV] Identity mappings in deep residual networks 
  3. ^  [2019] An Empirical Study on Position of the Batch Normalization Layer in Convolutional Neural Networks 
  4. ^  [2020] Big Transfer (BiT)_ General Visual Representation Learning 
  5. ^  [2018 NeurIPS] How Does Batch Normalization Help Optimization? 

