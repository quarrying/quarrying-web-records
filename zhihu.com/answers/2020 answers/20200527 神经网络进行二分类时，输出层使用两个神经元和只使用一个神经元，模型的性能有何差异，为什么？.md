#! https://www.zhihu.com/question/397625619/answer/1249069226

[comment]: <> (Answer URL: https://www.zhihu.com/question/397625619/answer/1249069226)
[comment]: <> (Question Title: 神经网络进行二分类时，输出层使用两个神经元和只使用一个神经元，模型的性能有何差异，为什么？)
[comment]: <> (Author Name: 采石工)
[comment]: <> (Create Time: 2020-05-27 16:07:26)

理论上两者是没有差异的。分析如下：

1） 先引入 Logistic 函数和 softmax 函数的定义

logistic 函数定义为：$\operatorname{sigmoid} \left( x \right) = \frac{1}{1 + {e^{- x}}}$  。

logistic 函数还可以称为 sigmoid 函数，expit 函数（SciPy 中即采用此名称）。

softmax 函数定义为：  $\operatorname{softmax} \left( {\vec x} \right) =
\frac{{{{\left[ {\exp \left( x_0 \right), \cdots ,\exp \left( {{x_{n -
1}}} \right)} \right]}^T}}}{{\sum\nolimits_{i = 0}^{n - 1} {\exp \left(
{{x_i}} \right)} }}$  。

softmax 函数是多元向量值函数。

2）再引入 sigmoid 和 softmax 交叉熵损失函数的数学形式

设数据集  $S = \bigcup\nolimits_{i = 1}^N {\left\{ {\left\langle {{x_i},{y_i}}\right\rangle } \right\}}$ ,其中  ${y_i} \in \left\{ {0,1} \right\}$  为label，
${x_i}$  为 feature。

sigmoid 交叉熵损失函数的数学形式：

$$\operatorname{sigmoidloss} \left( S \right) = - \frac{1}{N}\sum\limits_{i
= 1}^N {\left[ {{y_i}\log \left( {\operatorname{sigmoid} \left( {{x_i}}
\right)} \right) + \left( {1 - {y_i}} \right)\log \left( {1 -
\operatorname{sigmoid} \left( {{x_i}} \right)} \right)} \right]}$$

softmax 交叉熵损失函数的数学形式：

$$\operatorname{softmaxloss} \left( S \right) = - \frac{1}{N}\sum\limits_{i
= 1}^N {{{\vec e}^T}\left( {{y_i}} \right){{\log }^ \circ
}\operatorname{softmax} \left( {{{\vec x}_i}} \right)}$$

其中  ${f^ \circ }\left( \cdot \right)$  表示按元素函数，例如  ${\exp ^ \circ }\left(
{\vec x} \right) = {\left[ {\exp \left( x_0 \right){\text{,}}\exp \left(
x_1 \right){\text{,}} \cdots {\text{,}}\exp \left( {{x_{n - 1}}} \right)}
\right]^T}$  。  $\vec e\left( y \right) = {\left[ {1\left\{ {y = 0}
\right\}, \cdots ,1\left\{ {y = n - 1} \right\}} \right]^T}$  是标签 y 的 one-
hot 编码，  $1\left\{ {{x_i} = {x_j}} \right\} = \left\{
{\begin{matrix}  1&{{x_i} = {x_j}} \\ 0&{{x_i} \ne {x_j}}
\end{matrix}} \right.$  是指示函数。


3）下面推导对于二分类上述两个 loss 的等效性

当  $n = 2$  时，因为  $1\left\{ {y = 0} \right\} = \left\{
{\begin{matrix}  1&{y = 0} \\ 0&{y \ne 0} \end{matrix}} \right. =
\left\{ {\begin{matrix}  1&{y = 0} \\ 0&{y = 1} \end{matrix}} \right.
= 1 - y$  ，同理  $1\left\{ {y = 1} \right\} = y$  ，所以  $\vec e\left( y \right)
= {\left[ {1 - {y},{y}} \right]^T}$  。令  $\vec x = {\left[ {x_0,x_1} \right]^T}$，则

$\operatorname{softmax} \left( {\vec x} \right) = \frac{{{{\left[ {\exp \left(
x_0 \right),\exp \left( x_1 \right)} \right]}^T}}}{{\exp \left(
x_0 \right) + \exp \left( x_1 \right)}} = 
\begin{bmatrix}
{1 - \mathrm{sigmoid}\left( {x_1 - x_0} \right)} \\ 
{\mathrm{sigmoid}\left( {x_1 - x_0} \right)} 
\end{bmatrix}$  。

所以

$$\begin{aligned} \operatorname{softmaxloss} \left( S \right) &= -
\frac{1}{N}\sum\nolimits_{i = 1}^N {{{\vec e}^T}\left( {{y_i}} \right){{\log
}^ \circ }\operatorname{softmax} \left( {{{\vec x}_i}} \right)} \\ &=
- \frac{1}{N}\sum\nolimits_{i = 1}^N {\left( {1 - {y_i}} \right)\log \left( {1
- \operatorname{sigmoid} \left( {{x_{{\text{1,}}i}} - {x_{{\text{0,}}i}}}
\right)} \right) + {y_i}\log \operatorname{sigmoid} \left( {{x_{{\text{1,}}i}}
- {x_{{\text{0,}}i}}} \right)}  \\ \end{aligned}$$

这在形式上已经和  $\operatorname{sigmoidloss} \left( S \right)$ 一样了。又由于深度学习可以自动学习特征，在最终性能上可以认为两者是等效的。

